{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 13. In-Class Practice\n",
    "## 修真聊天群的 word2vec 實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "import gensim\n",
    "import warnings\n",
    "from ipywidgets import IntProgress\n",
    "from hanziconv import HanziConv\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text lines: 213871\n"
     ]
    }
   ],
   "source": [
    "file_train_read = []\n",
    "with open('./dataset/xiuzhenliaotianqun.txt', encoding='utf-8') as file_train_raw:\n",
    "    for line in file_train_raw:\n",
    "        if line.strip() != '':\n",
    "            file_train_read.append(HanziConv.toTraditional(line.strip()))\n",
    "\n",
    "print(\"Text lines:\", len(file_train_read))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords number: 745\n"
     ]
    }
   ],
   "source": [
    "stopwords = set()\n",
    "with open('./dataset/stopwords.txt') as stopword_file:\n",
    "    for words in stopword_file:\n",
    "        stopwords.add(HanziConv.toTraditional(words.strip()))\n",
    "\n",
    "print(\"Stopwords number:\", len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a0ff7bfa4c489b93911a249df5eadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='[ 0 / 213871 ]', max=213871)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progress = IntProgress(min=0, max=len(file_train_read))\n",
    "progress.value = 0\n",
    "progress.description = \"[ %s / %s ]\"%(str(progress.value), str(progress.max))\n",
    "display(progress)\n",
    "\n",
    "file_train_seg = []\n",
    "for i in range(len(file_train_read)):\n",
    "    file_train_seg.append([' '.join([word for word in jieba.cut(file_train_read[i], cut_all=False)\n",
    "                                            if word not in stopwords])])\n",
    "    progress.value +=1\n",
    "    progress.description = \"[ %s / %s ]\"%(str(progress.value), str(progress.max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "季節 江南 地區 晝夜 溫差 變 很大 白天 還穿 褲衩 熱成 狗 晚上 縮 窩 裏 凍成 寒號 鳥\n"
     ]
    }
   ],
   "source": [
    "# 將jieba的斷詞產出存檔\n",
    "file_seg_word_done_path = 'corpus_seg_done.txt'\n",
    "with open(file_seg_word_done_path, 'wb') as f:\n",
    "    for i in range(len(file_train_seg)):\n",
    "        f.write((file_train_seg[i][0] + '\\n').encode('utf-8'))\n",
    "\n",
    "# 檢視斷詞 jieba 的結果\n",
    "def print_list_chinese(list):\n",
    "    for i in range(len(list)):\n",
    "        print(list[i])\n",
    "        \n",
    "print_list_chinese(file_train_seg[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevink556\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "sentences = word2vec.LineSentence(file_seg_word_done_path)\n",
    "model = word2vec.Word2Vec(sentences, size=250)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevink556\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.37159345,  0.36440477, -0.68283176, -0.3977087 , -1.2312399 ,\n",
       "         0.19338082,  0.3192118 , -0.03781476,  1.0747726 , -0.7108403 ],\n",
       "       [ 0.13085344, -0.3223434 ,  1.049067  , -0.2065799 ,  0.05534005,\n",
       "        -0.635839  ,  0.27663794,  0.11506622, -0.37989545,  1.6684337 ],\n",
       "       [ 0.368835  ,  0.2109844 , -0.7678621 ,  0.33032176, -0.38054812,\n",
       "        -1.0027988 , -1.6532946 , -0.05391168, -0.02363169,  0.08944174],\n",
       "       [ 1.2138239 ,  1.2577821 , -0.36262798,  0.34616736,  0.79602545,\n",
       "         0.09819802, -1.0791831 , -0.49653545, -1.2262968 , -0.9167704 ],\n",
       "       [-0.6844266 , -0.3682592 , -0.43402216, -0.6814844 , -0.22692993,\n",
       "         0.67157906,  0.27216297,  0.10316256, -0.77437454, -0.63960934],\n",
       "       [-0.09675064,  0.30531484,  0.7468311 , -0.2703993 ,  0.7651356 ,\n",
       "         0.5088317 ,  0.20411667, -0.17121835, -0.0575964 , -0.09510022],\n",
       "       [-1.4711207 ,  0.0647627 , -0.33499458,  0.7418404 ,  0.43193376,\n",
       "         1.6202635 ,  0.7939597 , -0.45636332, -0.11744399, -1.8806119 ],\n",
       "       [-0.29798394, -0.18186605, -1.7662848 ,  0.4660325 , -0.5902055 ,\n",
       "        -0.38414234,  0.0233522 , -0.06686531, -0.35131508, -1.0313228 ],\n",
       "       [-1.583125  ,  0.20700477, -1.4599504 ,  0.5632619 , -1.1112355 ,\n",
       "         0.09555832, -0.11911184, -1.7122614 , -0.3868131 , -2.812085  ],\n",
       "       [ 0.7189275 ,  0.19880912,  0.7603924 ,  1.455866  ,  1.0644609 ,\n",
       "        -0.35862747,  0.82628083, -0.41232982, -0.98446023,  0.06852881]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以用 model.wv.syn0 檢視詞向量，因為一個詞有250個維度，全部列出過於冗長.....\n",
    "# 這邊僅僅呈現 10 個詞的 10 個維度\n",
    "\n",
    "model.wv.syn0[0:10,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "之處\n",
      "元\n",
      "想想\n",
      "如今\n",
      "賬號\n"
     ]
    }
   ],
   "source": [
    "# 可檢視第 996~1000 個字詞是什麼\n",
    "\n",
    "for i in range(995,1000):\n",
    "    print(model.wv.index2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宋書航 相近詞： ['書航', '赤瞳', '宋航', '將書航', '瞭書航', '由書航', '李教員', '蔥娘', '小彩', '葉思'] \n",
      "\n",
      "修真 相近詞： ['資料', '本次', '是否', '主要', '權限', '加入', '升級', '共享', '私人', '二號'] \n",
      "\n",
      "法術 相近詞： ['能力', '道術', '術', '催眠', '天賦', '技能', '幻術', '小法術', '秘法', '手法'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('書航', 0.664),\n",
       " ('赤瞳', 0.603),\n",
       " ('宋航', 0.563),\n",
       " ('將書航', 0.522),\n",
       " ('瞭書航', 0.513),\n",
       " ('由書航', 0.478),\n",
       " ('李教員', 0.475),\n",
       " ('蔥娘', 0.473),\n",
       " ('小彩', 0.469),\n",
       " ('葉思', 0.469)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 顯示空間距離相近的詞\n",
    "print(\"宋書航 相近詞：\", [i[0] for i in model.similar_by_vector('宋書航')], '\\n')\n",
    "print(\"修真 相近詞：\", [i[0] for i in model.similar_by_vector('修真')], '\\n')\n",
    "print(\"法術 相近詞：\", [i[0] for i in model.similar_by_vector('法術')], '\\n')\n",
    "\n",
    "# 顯示相近詞和詞向量，直接使用 similar_by_vector() 就可以了\n",
    "model.similar_by_vector('宋書航')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回過頭來看一下詞向量模型的結果。\n",
    "\n",
    "首先我們用咱們男主角的名字「宋書航」丟進去測試，原則上要跑出一些主線人物的名字，畢竟他們伴隨著男主角成長，比較可能存在類似的脈絡中而被模型捕捉到。乍看也還算合理，不過「瞭書航」、「由書航」這二個詞很明顯地就是由簡轉繁引發的斷詞失誤了。\n",
    "\n",
    "第二個嘗試看看「修真」的相近詞，跑出來「資料」、「權限」、「共享」等詞彙，再度懷疑是斷詞引擎或者詞向量模型出了問題。其實不然，有看過這部小說的同學就會知道這部作品是多麼的不(ㄋㄠˇ) 落(ㄉㄨㄥˋ) 俗(ㄉㄚˋ) 套(ㄎㄞ)。這個例子再度告訴我們，Domain Knowlwdge 的重要性啊 XDDD。\n",
    "\n",
    "最後看看「法術」的相近詞，嗯，終於合乎我們的預期。這些字詞都是玄幻修真類小說的常用語彙，成功～\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不過還有一個小麻煩，同樣關乎斷詞引擎的失誤，就是當我們輸入一個很重要的角色「白前輩」進去搜尋相近詞時，居然找不到！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word '白前輩' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-1afb698b8e32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'白前輩'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1420\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1421\u001b[0m                 )\n\u001b[1;32m-> 1422\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1424\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36msimilar_by_vector\u001b[1;34m(self, vector, topn, restrict_vocab)\u001b[0m\n\u001b[0;32m   1432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1433\u001b[0m         \"\"\"\n\u001b[1;32m-> 1434\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1436\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method will be removed in 4.0.0, use self.wv.doesnt_match() instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36msimilar_by_vector\u001b[1;34m(self, vector, topn, restrict_vocab)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m         \"\"\"\n\u001b[1;32m--> 596\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msimilarity_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonzero_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word '白前輩' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.similar_by_vector('白前輩')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這邊我們有兩個問題要修正，一是關於簡轉繁後的判讀失誤，二是人名的錯誤斷詞。第一個問題，我們可以先讓斷詞引擎斷完，我們再翻譯語料庫中的詞彙。第二個問題我們則要加入相關字詞的字庫來調整斷詞。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
