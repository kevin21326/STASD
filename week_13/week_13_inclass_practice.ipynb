{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 13. In-Class Practice\n",
    "## 修真聊天群的 word2vec 實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevink556\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "import gensim\n",
    "import warnings\n",
    "from ipywidgets import IntProgress\n",
    "from hanziconv import HanziConv\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text lines: 213871\n"
     ]
    }
   ],
   "source": [
    "file_train_read = []\n",
    "with open('./dataset/xiuzhenliaotianqun.txt', encoding='utf-8') as file_train_raw:\n",
    "    for line in file_train_raw:\n",
    "        if line.strip() != '':\n",
    "            file_train_read.append(HanziConv.toTraditional(line.strip()))\n",
    "\n",
    "print(\"Text lines:\", len(file_train_read))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords number: 745\n"
     ]
    }
   ],
   "source": [
    "stopwords = set()\n",
    "with open('./dataset/stopwords.txt', encoding='utf-8') as stopword_file:\n",
    "    for words in stopword_file:\n",
    "        stopwords.add(HanziConv.toTraditional(words.strip()))\n",
    "\n",
    "print(\"Stopwords number:\", len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800c0004267241afa2888a87e81cda7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='[ 0 / 213871 ]', max=213871)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\KEVINK~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.077 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "progress = IntProgress(min=0, max=len(file_train_read))\n",
    "progress.value = 0\n",
    "progress.description = \"[ %s / %s ]\"%(str(progress.value), str(progress.max))\n",
    "display(progress)\n",
    "\n",
    "file_train_seg = []\n",
    "for i in range(len(file_train_read)):\n",
    "    file_train_seg.append([' '.join([word for word in jieba.cut(file_train_read[i], cut_all=False)\n",
    "                                            if word not in stopwords])])\n",
    "    progress.value +=1\n",
    "    progress.description = \"[ %s / %s ]\"%(str(progress.value), str(progress.max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "季節 江南 地區 晝夜 溫差 變 很大 白天 還穿 褲衩 熱成 狗 晚上 縮 窩 裏 凍成 寒號 鳥\n"
     ]
    }
   ],
   "source": [
    "# 將jieba的斷詞產出存檔\n",
    "file_seg_word_done_path = 'corpus_seg_done.txt'\n",
    "with open(file_seg_word_done_path, 'wb') as f:\n",
    "    for i in range(len(file_train_seg)):\n",
    "        f.write((file_train_seg[i][0] + '\\n').encode('utf-8'))\n",
    "\n",
    "# 檢視斷詞 jieba 的結果\n",
    "def print_list_chinese(list):\n",
    "    for i in range(len(list)):\n",
    "        print(list[i])\n",
    "        \n",
    "print_list_chinese(file_train_seg[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevink556\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "sentences = word2vec.LineSentence(file_seg_word_done_path)\n",
    "model = word2vec.Word2Vec(sentences, size=250)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevink556\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.7968492 , -0.16908197, -0.80101156, -0.78538275,  0.47682828,\n",
       "        -0.01210073,  0.4343004 , -0.8885662 , -0.7434057 , -0.69766587],\n",
       "       [ 0.5947968 , -0.6265896 ,  0.4628336 ,  0.05538532, -0.38051885,\n",
       "        -0.68068534, -1.134348  ,  0.33785427,  0.14968921, -0.50054204],\n",
       "       [ 0.28178832,  0.64006305, -0.21529683, -1.3555954 ,  1.0761099 ,\n",
       "        -0.20608024, -0.6259862 ,  0.30736363, -1.2582827 , -0.52524406],\n",
       "       [-0.467692  , -0.01088117,  0.94149107, -0.7858169 , -0.4444069 ,\n",
       "         0.02835883, -0.25186068,  0.829317  , -0.08071167, -0.74918634],\n",
       "       [ 0.13304971,  0.26200008, -1.0281827 ,  0.57304007, -0.23213628,\n",
       "         0.37781435,  1.4464221 ,  1.6496657 , -1.3580114 ,  1.9172757 ],\n",
       "       [ 1.3082304 ,  1.5606303 , -0.73164725, -0.6006969 ,  0.6776822 ,\n",
       "         1.2346125 ,  0.42586622, -0.61109245, -0.350493  , -0.752032  ],\n",
       "       [ 0.89260757,  0.64374447,  2.2064142 ,  1.6680992 , -0.6439324 ,\n",
       "        -1.128948  , -1.5579499 , -1.249654  ,  0.04970297, -0.01908335],\n",
       "       [-0.18806908, -1.0010799 , -0.6430391 , -0.01469418, -1.6252816 ,\n",
       "        -0.7675918 ,  3.5000284 ,  1.571604  ,  1.3587703 ,  2.465109  ],\n",
       "       [-0.4837883 ,  1.4477221 , -0.3462747 ,  0.16221522,  1.0057853 ,\n",
       "        -0.4672644 , -1.497949  , -0.32885137,  0.6104594 ,  0.77712196],\n",
       "       [-1.3047011 , -0.2792565 , -0.40895247,  0.7200659 ,  0.26299617,\n",
       "        -0.8093367 , -0.51049596, -1.2614857 , -0.77457285, -0.25782898]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以用 model.wv.syn0 檢視詞向量，因為一個詞有250個維度，全部列出過於冗長.....\n",
    "# 這邊僅僅呈現 10 個詞的 10 個維度\n",
    "\n",
    "model.wv.syn0[0:10,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高升\n",
      "丹藥\n",
      "起身\n",
      "路\n",
      "盔甲\n"
     ]
    }
   ],
   "source": [
    "# 可檢視第 996~1000 個字詞是什麼\n",
    "\n",
    "for i in range(995,1000):\n",
    "    print(model.wv.index2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevink556\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similar_by_vector` (Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宋書航 相近詞： ['書航', '赤瞳', '宋航', '將書航', '瞭書航', '葉思', '李教員', '小彩', '葉師姐', '蔥娘'] \n",
      "\n",
      "修真 相近詞： ['資料', '加入', '是否', '本次', '設定', '權限', '冒險', '私人', '共享', '更改'] \n",
      "\n",
      "法術 相近詞： ['道術', '能力', '小法術', '催眠', '術', '強力', '天賦', '幻術', '秘術', '秘法'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevink556\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "C:\\Users\\kevink556\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similar_by_vector` (Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\kevink556\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `similar_by_vector` (Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\kevink556\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `similar_by_vector` (Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('書航', 0.6530104279518127),\n",
       " ('赤瞳', 0.6222689151763916),\n",
       " ('宋航', 0.5915800333023071),\n",
       " ('將書航', 0.5310773253440857),\n",
       " ('瞭書航', 0.5094061493873596),\n",
       " ('葉思', 0.5038828253746033),\n",
       " ('李教員', 0.46544134616851807),\n",
       " ('小彩', 0.4620957374572754),\n",
       " ('葉師姐', 0.45875030755996704),\n",
       " ('蔥娘', 0.45868104696273804)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示空間距離相近的詞\n",
    "print(\"宋書航 相近詞：\", [i[0] for i in model.similar_by_vector('宋書航')], '\\n')\n",
    "print(\"修真 相近詞：\", [i[0] for i in model.similar_by_vector('修真')], '\\n')\n",
    "print(\"法術 相近詞：\", [i[0] for i in model.similar_by_vector('法術')], '\\n')\n",
    "\n",
    "# 顯示相近詞和詞向量，直接使用 similar_by_vector() 就可以了\n",
    "model.similar_by_vector('宋書航')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回過頭來看一下詞向量模型的結果。\n",
    "\n",
    "首先我們用咱們男主角的名字「宋書航」丟進去測試，原則上要跑出一些主線人物的名字，畢竟他們伴隨著男主角成長，比較可能存在類似的脈絡中而被模型捕捉到。乍看也還算合理，不過「瞭書航」、「由書航」這二個詞很明顯地就是由簡轉繁引發的斷詞失誤了。\n",
    "\n",
    "第二個嘗試看看「修真」的相近詞，跑出來「資料」、「權限」、「共享」等詞彙，再度懷疑是斷詞引擎或者詞向量模型出了問題。其實不然，有看過這部小說的同學就會知道這部作品是多麼的不(ㄋㄠˇ) 落(ㄉㄨㄥˋ) 俗(ㄉㄚˋ) 套(ㄎㄞ)。這個例子再度告訴我們，Domain Knowlwdge 的重要性啊 XDDD。\n",
    "\n",
    "最後看看「法術」的相近詞，嗯，終於合乎我們的預期。這些字詞都是玄幻修真類小說的常用語彙，成功～\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不過還有一個小麻煩，同樣關乎斷詞引擎的失誤，就是當我們輸入一個很重要的角色「白前輩」進去搜尋相近詞時，居然找不到！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word '白前輩' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-1afb698b8e32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'白前輩'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1420\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1421\u001b[0m                 )\n\u001b[1;32m-> 1422\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1424\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36msimilar_by_vector\u001b[1;34m(self, vector, topn, restrict_vocab)\u001b[0m\n\u001b[0;32m   1432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1433\u001b[0m         \"\"\"\n\u001b[1;32m-> 1434\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1436\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method will be removed in 4.0.0, use self.wv.doesnt_match() instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36msimilar_by_vector\u001b[1;34m(self, vector, topn, restrict_vocab)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m         \"\"\"\n\u001b[1;32m--> 596\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msimilarity_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonzero_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word '白前輩' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.similar_by_vector('白前輩')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這邊我們有兩個問題要修正，一是關於簡轉繁後的判讀失誤，二是人名的錯誤斷詞。第一個問題，我們可以先讓斷詞引擎斷完，我們再翻譯語料庫中的詞彙。第二個問題我們則要加入相關字詞的字庫來調整斷詞。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
